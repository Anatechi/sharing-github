{"cells":[{"metadata":{"_uuid":"2ae3c2d66c7b4dc3e1b34e50e8f24b7b77ee7a54"},"cell_type":"markdown","source":"# Pitch detection\n<a href=\"https://en.wikipedia.org/wiki/Pitch_(music)\">Pitch</a> detection from audio files.\n\nUsing [Scikit-Learn](https://scikit-learn.org):\n\n&nbsp;&nbsp;&nbsp;&nbsp;*Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.*\n\n&nbsp;&nbsp;&nbsp;&nbsp;*Scikit-learn: Machine Learning in Python, 2011*\n\nUsing [NSynth](https://magenta.tensorflow.org/datasets/nsynth) dataset:\n\n&nbsp;&nbsp;&nbsp;&nbsp;*Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi*\n\n&nbsp;&nbsp;&nbsp;&nbsp;*Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders, 2017*\n\nNumber of WAV files in dataset:\n- train = 289,205 examples\n- valid = 12,678 examples\n- test = 4,096 examples\n\n<span style=\"color:red\">For computational cost purpose, using only test dataset and splitting it into train/test sets</span>\n\nEach WAV file is 64000 samples at 16 kHz, which is to say 4 seconds."},{"metadata":{"trusted":true,"_uuid":"2a93d87ff9aee8da62d28c6222a037d8a85c7dba"},"cell_type":"code","source":"import zipfile\nimport os\nimport json\nimport random\nimport itertools as it\n\nimport numpy as np\nimport pandas as pd\n\nfrom scipy import signal\nfrom scipy.io import wavfile\n\nimport sklearn as sk\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c7ffd839e7e7e6ad5e6e00e6e4db5748f8a96a8"},"cell_type":"markdown","source":"## 1. Preprocessing"},{"metadata":{"_uuid":"dcaae01e091bbf5a45364b38832029e9911caa32"},"cell_type":"markdown","source":"### 1.1. Spectrogram\nMIDI pitch uses logarithmic scale. Thus spectrogram using 128 frequency buckets is not sufficient to distinguish between pitches, especially at beginning of scale.\n\nLowest pitches differ from each other from less than 4 Hz. This is the required resolution, and since MIDI range is 4200 Hz from lowest pitch to highest one, number of buckets should be 4200 / 4 = 1050. Using 1024 instead, see \"nperseg=1024\" when calling \"spectrogram\" method..\n\nThus for each WAV file, spectrogram = 1024 frequency buckets x 285 time buckets = 285 samples x 1024 features, and all samples of a given WAV file are associated to the same pitch label given in metadata."},{"metadata":{"_uuid":"03a88983eead938975adccf0821a52aa7efe1cdf"},"cell_type":"markdown","source":"#### 1.1.1. Computing spectrogram"},{"metadata":{"trusted":true,"_uuid":"52f338a3497dbb51aa0e9fdea1fdd26e2a8d57cd"},"cell_type":"code","source":"def get_spectrogram(file_name, remove_begin=0, remove_end=0, filter_intensity=0):\n    \"\"\"\n    Provide spectrogram associated to a WAV file\n    To focus only on relevant information:\n    - optionally removes begin and end of WAV file (ratio between 0 and 1)\n    - optionally removes time buckets whose frequency intensities are all under a given threshold (absolute)\n    \"\"\"\n\n    # Reading WAV file\n    sample_rate, samples = wavfile.read(file_name)\n    \n    # Removing begin and end of samples since they may not be relevant\n    length = len(samples)\n    \n    if remove_begin > 0:\n        \n        samples_to_remove = int(length * remove_begin)\n        samples = samples[samples_to_remove:]\n        \n    if remove_end > 0:\n        \n        samples_to_remove = int(length * remove_end)\n        samples = samples[:-samples_to_remove]\n    \n    # Computing spectogram\n    frequencies, times, spectrogram = signal.spectrogram(samples, fs=sample_rate, nperseg=1024)\n\n    # Getting max intensity for each time bucket\n    max_intensity = np.amax(spectrogram, axis=0)\n\n    # Filtering on max intensity\n    selections = np.array(max_intensity > filter_intensity)\n\n    return frequencies, times[selections], spectrogram[:, selections], max_intensity[selections], sample_rate, samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6bc42cba3904ba354ff8a0d6355c56c8ca37bef"},"cell_type":"code","source":"def display_spectrogram(frequencies, times, spectrogram, sample_rate, samples):\n    \"\"\"\n    Display spectrogram\n    \"\"\"\n    \n    fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n        \n    # Plotting frequencies for a given time\n    axs[0].plot(frequencies, spectrogram[:, 50])\n    axs[0].set_ylabel('Intensity')\n    axs[0].set_xlabel('Frequency [Hz]')\n    axs[0].set_title(\"Frequencies at arbitrary given time\")\n\n    # Plotting spectrogram (method 1)\n    axs[1].pcolormesh(times, frequencies, 10*np.log10(spectrogram))\n    axs[1].set_ylabel('Frequency [Hz]')\n    axs[1].set_xlabel('Time [sec]')\n    axs[1].set_title(\"Spectrogram (method 1)\")\n\n    # Plotting spectogram (method 2)\n    axs[2].specgram(samples, Fs=sample_rate, NFFT=25, noverlap=5, detrend='mean', mode='psd')\n    axs[2].set_ylabel('Frequency [Hz]')\n    axs[2].set_xlabel('Time [sec]')\n    axs[2].set_title(\"Spectrogram (method 2)\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d19ddacab3bddd55575e8747b178baf440435003"},"cell_type":"markdown","source":"#### 1.1.2. Visual check"},{"metadata":{"trusted":true,"_uuid":"65cc25245f239b1394bbdd9acb7771cb01760639"},"cell_type":"code","source":"# Testing getting spectrogram\nfrequencies, times, spectrogram, _, sample_rate, samples = get_spectrogram(\n    '../input/nsynth-test/nsynth-test/audio/keyboard_acoustic_004-031-050.wav',\n    remove_begin=0.0,\n    remove_end=0.0,\n    filter_intensity=0)\ndisplay_spectrogram(frequencies, times, spectrogram, sample_rate, samples)\n\nfrequencies, times, spectrogram, _, sample_rate, samples = get_spectrogram(\n    '../input/nsynth-test/nsynth-test/audio/vocal_synthetic_003-107-050.wav',\n    remove_begin=0.0,\n    remove_end=0.0,\n    filter_intensity=0)\ndisplay_spectrogram(frequencies, times, spectrogram, sample_rate, samples)\n\nfrequencies, times, spectrogram, _, sample_rate, samples = get_spectrogram(\n    '../input/nsynth-test/nsynth-test/audio/mallet_acoustic_062-025-100.wav',\n    remove_begin=0.0,\n    remove_end=0.0,\n    filter_intensity=0)\ndisplay_spectrogram(frequencies, times, spectrogram, sample_rate, samples)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b87dd0ee5f91436d9f5462974ef6a5f9f75cbf3"},"cell_type":"markdown","source":"#### 1.1.3. Testing removing begin and end of file"},{"metadata":{"trusted":true,"_uuid":"a652f342fd67c21f24b2f41ab07932e5ae9fd73a"},"cell_type":"code","source":"# Testing removing begin and end\n_, times_raw, _, max_raw, _, _ = get_spectrogram('../input/nsynth-test/nsynth-test/audio/mallet_acoustic_062-025-100.wav',\n                                                 remove_begin=0, remove_end=0, filter_intensity=0)\n\n_, times_shortened, _, max_shortened, _, _ = get_spectrogram('../input/nsynth-test/nsynth-test/audio/mallet_acoustic_062-025-100.wav',\n                                                             remove_begin=0.1, remove_end=0.1, filter_intensity=0)\n\nfig, axs = plt.subplots(2, 1, figsize=(10, 10))\n\naxs[0].plot(times_raw, max_raw, label='Raw')\naxs[1].plot(times_shortened, max_shortened, label='Shortened')\n\naxs[0].set_xlim(0, 6)\naxs[0].set_ylim(0, 5e6)\naxs[1].set_xlim(0, 6)\naxs[1].set_ylim(0, 5e6)\n\naxs[0].set_title('Raw')\naxs[1].set_title('Shortened (10% begin and 10% end)')\n\nfig.suptitle('Removing begin and end')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40456d39210791b99b4f9866aeebd2e5d9e0fba3"},"cell_type":"markdown","source":"#### 1.1.4. Testing filtering on sample max intensity"},{"metadata":{"trusted":true,"_uuid":"84c8f4a96e43a43e09cde45ea3603038786f070c"},"cell_type":"code","source":"# Testing filtering on max intensity\n_, times_raw, _, max_raw, _, _ = get_spectrogram('../input/nsynth-test/nsynth-test/audio/mallet_acoustic_062-025-100.wav',\n                                                 remove_begin=0, remove_end=0, filter_intensity=0)\n\n_, times_filtered, _, max_filtered, _, _ = get_spectrogram('../input/nsynth-test/nsynth-test/audio/mallet_acoustic_062-025-100.wav',\n                                                           remove_begin=0, remove_end=0, filter_intensity=2e6)\n\nfig=plt.figure(figsize=(10, 5))\n\nplt.plot(times_raw, max_raw, label='Raw')\nplt.plot(times_filtered, max_filtered, label='Filtered')\n\nplt.legend(loc='upper right')\n\nfig.suptitle('Filtering on max intensity')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"279e7f52059144a034a8d65c100215da658cb898"},"cell_type":"markdown","source":"### 1.2. Preparing data\nHere is an extract from metadata JSON file for a given WAV file:\n\n```json\n{\n    \"bass_synthetic_068-049-025\": {\n        \"qualities\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], \n        \"pitch\": 49, \n        \"note\": 217499, \n        \"instrument_source_str\": \"synthetic\", \n        \"velocity\": 25, \n        \"instrument_str\": \"bass_synthetic_068\", \n        \"instrument\": 656, \n        \"sample_rate\": 16000, \n        \"qualities_str\": [\"dark\"], \n        \"instrument_source\": 2, \n        \"note_str\": \"bass_synthetic_068-049-025\", \n        \"instrument_family\": 0, \n        \"instrument_family_str\": \"bass\"\n    },\n    ```"},{"metadata":{"trusted":true,"_uuid":"408bd44c94d7682eb8a464dfe1dd7f967954fa18"},"cell_type":"code","source":"# Reading JSON file containing metadata\njson_metadata = open('../input/nsynth-test/nsynth-test/examples.json').read()\nmetadata = json.loads(json_metadata)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25137bf69ab2751a8511ecf275c17bd7ad71f3ce"},"cell_type":"markdown","source":"Preparing data consists in:\n- computing spectrogram of each WAV file and associating pitch from metadata\n- scaling\n- shuffling dataset\n- splitting dataset into train/test sets"},{"metadata":{"trusted":true,"_uuid":"602a03cc76e8b4dc08a92843b2d0672d66f20ab3"},"cell_type":"code","source":"def prepare_data(file_name, samples, labels):\n    \n    # Getting spectrogram\n    # Frequencies are frequency buckets\n    # Times are time buckets\n    # Intensities are intensities for each (frequency bucket, time bucket) couple\n    # Max intensities are max intensities for each time bucket\n    frequencies, times, intensities, max_intensities, _, _ = get_spectrogram(\n        file_name,\n        remove_begin=REMOVE_BEGIN,\n        remove_end=REMOVE_END,\n        filter_intensity=FILTER_INTENSITY)\n\n    # Transposing spectrogram, to switch from frequencies x times to times x frequencies\n    intensities = intensities.transpose()\n\n    # Concatenating all time buckets in samples and labels sets\n    # A time bucket is a list of 129 frequencies like [2.93450565e+01 5.87889600e+03 1.26233027e+04 2.72070879e+04 ... ]\n    for time_bucket in intensities:\n\n        samples.append(time_bucket)\n\n        if labels is not None:\n            \n            # Pitch comes from metadata\n            labels.append(value['pitch'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b06db8e3abb1d4dfb6119cb194f556bf2229527"},"cell_type":"code","source":"REMOVE_BEGIN=0.0\nREMOVE_END=0.0\nFILTER_INTENSITY=1e6\n\nsamples = []\nlabels = []\n    \n# Looping on metadata\n# Key would be something like \"bass_synthetic_068-049-025\"\n# Value like {\"qualities\": [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], \"pitch\": 49, \"note\": 217499 ...\nfor key, value in metadata.items():\n\n    prepare_data('../input/nsynth-test/nsynth-test/audio/' + key + '.wav', samples, labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38af5bfba93a0e8273ed9c5b886a327e6043a3d4"},"cell_type":"markdown","source":"Scaling"},{"metadata":{"trusted":true,"_uuid":"3b03f9baa196fbe241d7296addbfb7ffad0082fa"},"cell_type":"code","source":"# Scaling samples\nscaler = StandardScaler().fit(samples)\nsamples = scaler.transform(samples)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f57508a92b9500b2636a9b543170a3a92e49d833"},"cell_type":"markdown","source":"Shuffling and splitting"},{"metadata":{"trusted":true,"_uuid":"af067463694a8271c17ba388582bf0074d334d64"},"cell_type":"code","source":"# Shuffling and splitting, 1 split only at the beginning\nshuffle_split = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)\nshuffle_split.get_n_splits(samples)\n\nfor train_index, test_index in shuffle_split.split(samples):\n    \n    samples_train = np.asarray(samples)[train_index]\n    samples_test = np.asarray(samples)[test_index]\n    labels_train = np.asarray(labels)[train_index]\n    labels_test = np.asarray(labels)[test_index]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa05e9b9f63eccbf94f588aee10cc2f9b38d189a"},"cell_type":"markdown","source":"### 1.3. Analyzing samples"},{"metadata":{"_uuid":"143a7f9397a0d191c064d75d704e8ea7e5a0227a"},"cell_type":"markdown","source":"#### 1.3.1. Distribution\nLooking for output classes (frequencies) without corresponding sample"},{"metadata":{"trusted":true,"_uuid":"8b89ef701b105bcf960dc46e465ae6ad954fb51d"},"cell_type":"code","source":"# Computing train set distribution\ndistribution = [key for key, group in it.groupby(sorted(labels_train), key=lambda x:x)]\n\n# Getting frequencies without corresponding sample\n# 128 stands for number of different classes in labels\nprint(set(range(1, 129)) - set(distribution))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75217af9d287cd3bd386559231ccbea84a329a5a"},"cell_type":"markdown","source":"Displaying distribution"},{"metadata":{"trusted":true,"_uuid":"fd37bd16cf3d8922a863b3bc99be1b0592351966"},"cell_type":"code","source":"# Displaying train set distribution\n# 128 stands for number of different classes in labels\nfig=plt.figure(figsize=(10, 3))\nplt.hist(labels_train, bins=128)\nfig.suptitle('Labels distribution in train set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a9938e2c7243faae53a5baadede8590c626f585"},"cell_type":"markdown","source":"#### 1.3.2. Visual check\nLooking at samples randomly chosen"},{"metadata":{"trusted":true,"_uuid":"38fb9a2e0b2171ee274aff95b040407156947f96"},"cell_type":"code","source":"for _ in range(1, 4):\n    \n    sample_index = random.randint(0, len(samples_train))\n    fig=plt.figure(figsize=(10, 3))\n    plt.plot(samples_train[sample_index])\n    fig.suptitle(labels_train[sample_index])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a38bbb9d70777b941f41bbf50fb8c7fa03f24073"},"cell_type":"markdown","source":"## 2. Learn"},{"metadata":{"_uuid":"27d24cc5ab9121757bb935ef7b27a871424c19e1"},"cell_type":"markdown","source":"### 2.1. Logistic regression"},{"metadata":{"trusted":true,"_uuid":"1ce9bbdb48fb04c5db2062af8261a6b550d9c735"},"cell_type":"code","source":"clf_logistic = LogisticRegression(random_state=0,\n                                  solver='lbfgs',\n                                  multi_class='multinomial',\n                                  max_iter=500).fit(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"560b570fd91c79e31fb060d3c05b3974950397d9"},"cell_type":"code","source":"clf_logistic.score(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ea3817218d2e3b43c661ebe881467d2579b5db9"},"cell_type":"code","source":"clf_logistic.score(samples_test, labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2be84179c4c7380f866b1fbf4de7c3a63f85669e"},"cell_type":"markdown","source":"### 2.2. SVM"},{"metadata":{"_uuid":"9c21e0b9d00c5c2a043372055e50cd60da56d023"},"cell_type":"markdown","source":"#### 2.2.1. Linear\nHigh computational cost"},{"metadata":{"trusted":true,"_uuid":"22b5ca6130050995d8c899420b8b52a597b9bb5b"},"cell_type":"code","source":"#clf_linear_SVC = LinearSVC(C=1.0,\n#                           class_weight=None,\n#                           dual=False,\n#                           fit_intercept=True,\n#                           intercept_scaling=1,\n#                           loss='squared_hinge',\n#                           max_iter=200,\n#                           multi_class='ovr', \n#                           penalty='l2', \n#                           random_state=0, \n#                           tol=1e-05, \n#                           verbose=0).fit(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51949ea83c1736e8ea2a1b9f565c80a9c7c224d5"},"cell_type":"code","source":"#clf_linear_SVC.score(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b24204a43274cfc869aa4a573b5dcb774c67095"},"cell_type":"code","source":"#clf_linear_SVC.score(samples_test, labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bcc6995cf4b64523cd152108c71a91731bf19c1"},"cell_type":"markdown","source":"#### 2.2.2. RBF kernel\nHigh computational cost"},{"metadata":{"trusted":true,"_uuid":"a73524350b4803c36b3b930613f9a67c2ec3e99d"},"cell_type":"code","source":"#clf_SVC = SVC(kernel='rbf',\n#              gamma=0.7,\n#              C=1.0).fit(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"437e0de2272c26344bf75a759a007d37b35fa889"},"cell_type":"code","source":"#clf_SVC.score(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"32125657f6e4841f62bad777dd02636e25663276"},"cell_type":"code","source":"#clf_SVC.score(samples_test, labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e2b71d7a9a2e0f60c585b64cd3250fc02289f0"},"cell_type":"markdown","source":"### 2.3. Neural network"},{"metadata":{"trusted":true,"_uuid":"089992b261c229d3677f6e4926b7928e9729fe25"},"cell_type":"code","source":"clf_neural = MLPClassifier(solver='lbfgs',\n                           alpha=1e-5,\n                           hidden_layer_sizes=(16),\n                           random_state=1).fit(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99f8e667af3e3e2439fff4023e912d720065c6e6"},"cell_type":"code","source":"clf_neural.score(samples_train, labels_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32681c06579a34a6619b0649e420ab62d47d5f0c"},"cell_type":"code","source":"clf_neural.score(samples_test, labels_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a5a69dbc0dfe6943ebf89f8d3c4135b58b0e4d9"},"cell_type":"markdown","source":"### 2.4. Error analysis"},{"metadata":{"_uuid":"c96e2bdd9b616f348ea84ea37690a44554341709"},"cell_type":"markdown","source":"#### 2.4.1. Number of samples\nHow much do we benefit from adding more training data ?\n\nDoes the estimator suffers more from a variance error or a bias error ?\n\nIf both validation score and training score converge to a low value, we won't benefit from more training data."},{"metadata":{"trusted":true,"_uuid":"776830edd7013decf5754b2cb41c76a99a84a97e"},"cell_type":"code","source":"train_sizes, train_scores, valid_scores = learning_curve(\n    MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(16), random_state=1),\n    samples,\n    labels,\n    train_sizes=[100, 1000, 5000, 20000, 50000],\n    cv=3,\n    n_jobs=3,\n    shuffle=True)\n\n# Plotting learning curve for both train scores and test scores\nplt.plot(train_sizes, train_scores[:, 0], c='b', label='Train - Fold 0')\nplt.plot(train_sizes, train_scores[:, 1], c='g', label='Train - Fold 1')\nplt.plot(train_sizes, train_scores[:, 2], c='r', label='Train - Fold 2')\nplt.plot(train_sizes, valid_scores[:, 0], c='c', label='Test - Fold 0')\nplt.plot(train_sizes, valid_scores[:, 1], c='m', label='Test - Fold 1')\nplt.plot(train_sizes, valid_scores[:, 2], c='y', label='Test - Fold 2')\nplt.legend(loc='lower right');\nplt.title = 'Learning curves'\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa2761cf7b2656a1ddbc06a8aa13e5f7b1d7095c"},"cell_type":"markdown","source":"#### 2.4.2. Looking at errors"},{"metadata":{"trusted":true,"_uuid":"dc4a45617ddd860921fdafdc86f21fae004b4e40"},"cell_type":"code","source":"# Getting predictions\npredictions = clf_neural.predict(samples_test)\n\n# Getting indexes where predictions differ from ground truth\nerror_indexes = (predictions !=  labels_test)\n\n# Computing error for each of these indexes\ndelta = predictions[error_indexes] - labels_test[error_indexes]\ndelta = [abs(number) for number in delta]\n\n# Computing average of these errors\n# 128 stands for number of different classes in labels\nprint(sum(delta) / len(delta) / 128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30ebd65008ac970c229a22bd4524ebed485b48d0"},"cell_type":"code","source":"# Displaying one of these errors\nplt.plot(samples_test[error_indexes][0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"706d56075d74607122c77352b80d99e223cbffb7"},"cell_type":"markdown","source":"## 3. Prediction"},{"metadata":{"trusted":true,"_uuid":"e84c6a61105adeca01bd34bfe5b2eabcdd4419c1"},"cell_type":"code","source":"samples_prediction = []\n\n# Preprocessing\nprepare_data('../input/nsynth-test/nsynth-test/audio/guitar_acoustic_010-056-050.wav', samples_prediction, None)\nsamples_prediction_scaled = scaler.transform(samples_prediction)\nsamples_prediction_scaled = np.asarray(samples_prediction_scaled)\n\n# Prediction\nclf_neural.predict(samples_prediction_scaled)\n\n# Comparison with ground truth is easy since it is embedded in filename xxxxx_xxx-056-xxx.wav","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ff9c1d7e7d13483ac25c07b56b9f9c345bcd2bc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}